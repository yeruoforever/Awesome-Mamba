# Awesome-Mamba
Awsome works based on SSM and Mamba

## SSMs

### Blog

* [A Visual Guide to Mamba and State Space Models (MAARTEN GROOTENDORST)](https://maartengrootendorst.substack.com/p/a-visual-guide-to-mamba-and-state)
* [Introduction to State Space Models (SSM) (lbourdois
Lo√Øck BOURDOIS)](https://huggingface.co/blog/lbourdois/get-on-the-ssm-train)

### Post

* [The Annotated S4](https://srush.github.io/annotated-s4/)
* [Structured State Spaces for Sequence Modeling (S4) part 1](https://hazyresearch.stanford.edu/blog/2022-01-14-s4-1)
* [Structured State Spaces for Sequence Modeling (S4) part 2](https://hazyresearch.stanford.edu/blog/2022-01-14-s4-2)
* [Structured State Spaces for Sequence Modeling (S4) part 3](https://hazyresearch.stanford.edu/blog/2022-01-14-s4-3)
* [HiPPO: Recurrent Memory with Optimal Polynomial Projections](https://hazyresearch.stanford.edu/blog/2020-12-05-hippo)
* [H3: Language Modeling with State Space Models and (Almost) No Attention](https://hazyresearch.stanford.edu/blog/2023-01-20-h3)

|Title|Year|Venue|Code|PDF|
|---|---|---|---|---|
|[Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling](https://arxiv.org/abs/2402.10211)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2402.10211)|
|[Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data](https://arxiv.org/abs/2402.05892)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2402.05892)|
|[Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks](https://arxiv.org/abs/2402.04248)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2402.04248)|
|[Is Mamba Capable of In-Context Learning?](https://arxiv.org/abs/2402.03170)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2402.03170)|
|[BlackMamba: Mixture of Experts for State-Space Models](https://arxiv.org/abs/2402.01771)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2402.01771)|
|[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)|2023|arXiv| code |[pdf](https://arxiv.org/abs/2312.00752)|
|[Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors](https://arxiv.org/abs/2310.02980)|2023|arXiv| code |[pdf](https://arxiv.org/abs/2310.02980)|
|[What Makes Convolutional Models Great on Long Sequence Modeling?](https://arxiv.org/abs/2210.09298)|2023|ICLR 2023| code |[pdf](https://arxiv.org/abs/2210.09298)|
|[Mega: Moving Average Equipped Gated Attention](https://arxiv.org/abs/2209.10655)|2023|ICLR 2023| code |[pdf](https://arxiv.org/abs/2209.10655)|
|[Liquid Structural State-Space Models](https://arxiv.org/abs/2209.12951)|2023|ICLR 2023| code |[pdf](https://arxiv.org/abs/2209.12951)|
|[Hungry Hungry Hippos: Towards Language Modeling with State Space Models](https://arxiv.org/abs/2212.14052)|2022|ICLR 2023| code |[pdf](https://arxiv.org/abs/2212.14052)|
|[On the Parameterization and Initialization of Diagonal State Space Models](https://arxiv.org/abs/2206.11893)|2022|NeurIPS 2022| code |[pdf](https://arxiv.org/abs/2206.11893)|
|[S4ND: Modeling Images and Videos as Multidimensional Signals Using State Spaces](https://arxiv.org/abs/2210.06583)|2022|NeurIPS 2022| [code](https://github.com/state-spaces/s4/tree/main/models/s4nd) |[pdf](https://arxiv.org/abs/2210.06583)|
|[Long Range Language Modeling via Gated State Spaces](https://arxiv.org/abs/2206.13947)|2022|ICLR 2023| code |[pdf](https://arxiv.org/abs/2206.13947)|
|[Simplified State Space Layers for Sequence Modeling](https://arxiv.org/abs/2208.04933)|2022|ICLR 2023| [code](https://github.com/lindermanlab/S5) |[pdf](https://arxiv.org/abs/2208.04933)|
|[How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections](https://arxiv.org/abs/2206.12037)|2022|ICLR 2023| code |[pdf](https://arxiv.org/abs/2206.12037)|
|[Diagonal State Spaces are as Effective as Structured State Spaces](https://arxiv.org/abs/2203.14343)|2022|NeurIPS 2022 - Spotlight| code |[pdf](https://arxiv.org/abs/2203.14343)|
|[Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396)|2021|ICLR 2022 - Outstanding Paper HM| [code](https://github.com/state-spaces/s4/tree/main/models/s4) |[pdf](https://arxiv.org/abs/2111.00396)|
|[Combining Recurrent, Convolutional, and Continuous-time Models with the Linear State Space Layer](https://arxiv.org/abs/2110.13985)|2021|NeurIPS 2021| code |[pdf](https://arxiv.org/abs/2110.13985)|
|[HiPPO: Recurrent Memory with Optimal Polynomial Projections](https://arxiv.org/abs/2008.07669)|2020|NeurIPS 2020 - Spotlight| [code](https://github.com/state-spaces/s4/tree/main/models/hippo) |[pdf](https://arxiv.org/abs/2008.07669)|



## Time Series

### Audio generation

|Title|Year|Venue|Code|PDF|
|---|---|---|---|---|
|[It's Raw! Audio Generation with State-Space Models](https://arxiv.org/abs/2202.09729)|2022|ICML 2022 - Long Talk| [code](https://github.com/state-spaces/s4/tree/main/models/sashimi) |[pdf](https://arxiv.org/abs/2202.09729)|

##  NLP

|Title|Year|Venue|Code|PDF|
|---|---|---|---|---|
|[MambaByte: Token-free Selective State Space Model](https://arxiv.org/abs/2401.13660)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2401.13660)|
|[MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts](https://arxiv.org/abs/2401.04081)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2401.04081)|



## Graph Neural Network

|Title|Year|Venue|Code|PDF|
|---|---|---|---|---|
|[Graph Mamba: Towards Learning on Graphs with State Space Models](https://arxiv.org/abs/2402.08678)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2402.08678)|


## Computer Vision

|Title|Year|Venue|Code|PDF|
|---|---|---|---|---|
|[Pan-Mamba: Effective pan-sharpening with State Space Model](https://arxiv.org/pdf/2402.12192.pdf)|2024|arXiv| code |[pdf](https://arxiv.org/pdf/2402.12192.pdf)|
|[U-shaped Vision Mamba for Single Image Dehazing](https://arxiv.org/abs/2402.04139)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2402.04139)|
|[Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model](https://arxiv.org/abs/2401.09417)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2401.09417)|


### Foundation Model

|Title|Year|Venue|Code|PDF|
|---|---|---|---|---|
|[Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining](https://arxiv.org/abs/2402.03302)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2402.03302)|



### Medical Image Analysis

#### Medical Image Segmentation

|Title|Year|Venue|Code|PDF|
|---|---|---|---|---|
|[Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for Scribble-based Medical Image Segmentation](https://arxiv.org/abs/2402.10887)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2402.10887)|
|[P-Mamba: Marrying Perona Malik Diffusion with Mamba for Efficient Pediatric Echocardiographic Left Ventricular Segmentation](https://arxiv.org/pdf/2402.08506)|2024|arXiv| code |[pdf](https://arxiv.org/pdf/2402.08506)|
|[Semi-Mamba-UNet: Pixel-Level Contrastive Cross-Supervised Visual Mamba-based UNet for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2402.07245)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2402.07245)|
|[Mamba-UNet: UNet-Like Pure Visual Mamba for Medical Image Segmentation](https://arxiv.org/abs/2402.05079)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2402.05079)|
|[VM-UNet: Vision Mamba UNet for Medical Image Segmentation](https://arxiv.org/abs/2402.02491)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2402.02491)|
|[Vivim: a Video Vision Mamba for Medical Video Object Segmentation](https://arxiv.org/abs/2401.14168)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2401.14168)|
|[SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation](https://arxiv.org/abs/2401.13560)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2401.13560)|
|[U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation](https://arxiv.org/abs/2401.04722)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2401.04722)|









#### Medical Image Registration

|Title|Year|Venue|Code|PDF|
|---|---|---|---|---|
|[MambaMorph: a Mamba-based Backbone with Contrastive Feature Learning for Deformable MR-CT Registration](https://arxiv.org/abs/2401.13934)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2401.13934)|


#### Others

|Title|Year|Venue|Code|PDF|
|---|---|---|---|---|
|[FD-Vision Mamba for Endoscopic Exposure Correction](https://arxiv.org/abs/2402.06378)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2402.06378)|

### Point Cloud

|Title|Year|Venue|Code|PDF|
|---|---|---|---|---|
|[PointMamba: A Simple State Space Model for Point Cloud Analysis](https://arxiv.org/abs/2402.10739)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2402.10739)|


## Structulal Data

|Title|Year|Venue|Code|PDF|
|---|---|---|---|---|
|[MambaTab: A Simple Yet Effective Approach for Handling Tabular Data](https://arxiv.org/abs/2401.08867)|2024|arXiv| code |[pdf](https://arxiv.org/abs/2401.08867)|

## Competitor

|Title|Year|Venue|Code|PDF|
|---|---|---|---|---|
|[Gated Linear Attention Transformers with Hardware-Efficient Training](https://arxiv.org/abs/2312.06635)|2023|arXiv| code |[pdf](https://arxiv.org/abs/2312.06635)|

